{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Now that you have the basis of data manipulation and analysis, it's time to introduce you to Machine Learning algorithms!\n",
    "\n",
    "Today we will:\n",
    "- Have a brief introduction to different types of learning\n",
    "- Use a DecisionTree to create a model that **predicts** home price values.\n",
    "- Learn how to **measure** your **model quality**.\n",
    "- Learn ways to **improve** this quality.\n",
    "\n",
    "I encourage you to read official documentations. You will find links in this Notebook, but the best way to learn is by being curious. Keep track of your questions and try to find answers on the internet.\n",
    "\n",
    "# Vocabulary\n",
    "\n",
    "In data science, we do not use the terms `rows` and `columns` to refer to a dataset. \n",
    "* The rows are called **samples**. For example, in our dataset of US States, each line describing a State is a sample.\n",
    "* The columns are called **features**. Each feature describe a particular aspect of the sample. In the US States dataset, the State id, the State name, and the Geometry were all features. In other datasets (with different samples), it could have been the length of a sentence, the frequency of a word, the number of legs of an animal, a creation date, a GPS location, a True/False boolean...\n",
    "* The **dimension** of a dataset is its number of features. If a dataset have 3 dimensions, we can call it a *3-dimensional* dataset.\n",
    "\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "In **supervised** learning, we already have a dataset containing the right answers, and we train a model to predict the right answer for new and unseen samples. \n",
    "\n",
    "The \"right answers\" are contained in a column that we call the **target**.\n",
    "\n",
    "Supervised algorithms can be used for:\n",
    "- Prediction\n",
    "- Product recommendation\n",
    "- Classification\n",
    "    - Handwriting recognition\n",
    "    - Speech recognition\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "In **unsupervised** learning, we do not know the right answers beforehand. We have to use algorithms to explore and understand meaninful informations, detect patterns, etc.\n",
    "\n",
    "Unsupervised algorithms can be used for:\n",
    "- Clustering (divide a set of datas into `k` subgroups, called clusters)\n",
    "- Pattern recognition\n",
    "- Anomaly detection\n",
    "- Dimensionality Reduction (i.e. PCA & LDA)\n",
    "\n",
    "Both type of learnings can use methods based on Neural Networks, but will dig into that later.\n",
    "\n",
    "## Semi-supervised Learning\n",
    "\n",
    "A related variant of learning that makes use of both supervised and unsupervised techniques. \n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "A decision tree is a **supervised** machine learning algorithm for both **classification** and **regression** problems.\n",
    "\n",
    "The goal is to create a **model** (a tree) that predicts a value of a *target* using selected *features*.\n",
    "\n",
    "A decision tree builds upon iteratively asking questions (based on *features*) to partition data. \n",
    "\n",
    "In the Decision Tree example below, we have:\n",
    "* A target: is giving a loan to this person is risky or safe? (2 unique values)\n",
    "* 2 features: one is credit history, the other is the income.\n",
    "\n",
    "![Decision Tree Example](https://miro.medium.com/max/1362/1*lAzhAq7poWg2hUfOp6-rsQ.png)\n",
    "\n",
    "What you ask at each step is critical part and greatly influences the performance of a decision tree. \n",
    "\n",
    "Decision trees can be used to:\n",
    "* Explicit decision making.\n",
    "* Describe / explain data\n",
    "* Determine the importance of each feature\n",
    "* Do classification (if the target has multiple unique values) or regression task (if the target is a number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\"\"\"\n",
    "`import sklearn` does not automatically import every submodules, you may have to import them explicitely like above.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from jyquickhelper import add_notebook_menu\n",
    "print(\"Table of Contents\")\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Download the dataset\n",
    "\n",
    "* Download the [Melbourne Housing dataset](https://www.kaggle.com/anthonypino/melbourne-housing-market)\n",
    "* Unzip it in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load file \"Melbourne_housing_FULL.csv\"\n",
    "melbourne_data = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint to remove NA values: use `DataFrame.dropna()`. [See official documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove rows with N/A values.\n",
    "melbourne_data = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the target and the features\n",
    "\n",
    "By convention, we call the target variable `y`, and the features variable `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = melbourne_data.Price\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(melbourne_data.columns)\n",
    "\n",
    "features = ['Rooms', 'Bathroom', 'Landsize', \"Car\", \"YearBuilt\"]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = melbourne_data[features]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data in 2D\n",
    "\n",
    "We will use the PCA to\n",
    "\n",
    "1. Find the most important axis.\n",
    "2. Print the data using only those axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the PCA on X values.\n",
    "pca = _\n",
    "\n",
    "k = 2\n",
    "eig_pairs = [(feature, eigenvalue) for feature, eigenvalue in zip(features, pca.explained_variance_)]\n",
    "eig_pairs = sorted(eig_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, eigenvalue in eig_pairs:\n",
    "    print(f\"{feature} eigenvalue: {eigenvalue:.2e}\")\n",
    "    \n",
    "selected_axis = [x[0] for x in eig_pairs[:k]]\n",
    "selected_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell may take time to run since it will prints 8887 points.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "ax.scatter(X[selected_axis[0]],\n",
    "           X[selected_axis[1]],\n",
    "           marker=\"x\")\n",
    "\n",
    "if True:\n",
    "    for i, txt in enumerate(melbourne_data['Address']):\n",
    "        x_point = X[selected_axis[0]][i]\n",
    "        y_point = X[selected_axis[1]][i]\n",
    "        text = ax.annotate(txt, (x_point + .02, y_point))\n",
    "        text.set_alpha(.5)\n",
    "\n",
    "plt.title('Houses 2D Visualization')\n",
    "\n",
    "ax.set(xlabel=selected_axis[0],\n",
    "       ylabel=selected_axis[1])\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use DecisionTree model\n",
    "\n",
    "Specifying a number for `random_state` ensures you get the same results in each run. This is considered a good practice. You use any number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "melbourne_model = DecisionTreeRegressor(random_state=1)\n",
    "melbourne_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you'll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we'll make predictions for the first few rows of the training data to see how the predict function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making predictions for the following 5 houses:\")\n",
    "print(X.head())\n",
    "print(\"The predictions are:\")\n",
    "\n",
    "melbourne_model.predict(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the tree\n",
    "\n",
    "Using `sklearn.tree.plot_tree`. [See documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)\n",
    "\n",
    "As you can see, we define a `max_depth` parameter. The tree's depth is a measure of how many splits it makes before coming to a prediction. Here, we define it to 2 as we just want to see the first most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "sklearn.tree.plot_tree(melbourne_model,\n",
    "                       ax=ax,\n",
    "                       max_depth=2,\n",
    "                      feature_names=features)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also\n",
    "* Customize your tree\n",
    "* Export it to a PNG file (like you can with every plot from matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "sklearn.tree.plot_tree(melbourne_model,\n",
    "                       ax=ax,\n",
    "                       max_depth=3,\n",
    "                       feature_names=features,\n",
    "                       proportion=True,\n",
    "                       rotate=True,\n",
    "                       rounded=True,\n",
    "                       filled=True, \n",
    "                       fontsize=10)\n",
    "\n",
    "plt.savefig(\"beautiful_tree.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting in training and testing set\n",
    "\n",
    "In order to study if our predictive model is working well enough, we need to try it under new samples that the model did not see before.\n",
    "\n",
    "To simulate that, we create a **training** and a **testing** test. \n",
    "\n",
    "Usually, we keep 1/3 of our original data as the test set, and build our model using the 2/3 left. \n",
    "\n",
    "We could do the splitting ourselves, but sklearn implemented a method to ease our work: `sklearn.model_selection.train_test_split()` [See documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use Decision Tree on your training set to create your model.\n",
    "# Hint: We did that before we splitted the data, creating a new var X_train\n",
    "\n",
    "melbourne_model = DecisionTreeRegressor(random_state=1)\n",
    "trained_tree = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your model to predict your testing set.\n",
    "predictions = _\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the better trained tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use `sklearn.tree.plot_tree` to visualize your tree.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "sklearn.tree.plot_tree(trained_tree,\n",
    "                       max_depth=2,\n",
    "                       feature_names=features)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Model\n",
    "\n",
    "To measure the model quality, a task also named *model validation*, we use **metrics**.\n",
    "\n",
    "One of the most common metric is **accuracy**.\n",
    "\n",
    "There are many metrics to compare model predictions with actual forecasts, but we'll start with one called **Mean Absolute Error** (also called MAE). \n",
    "\n",
    "The MAE is computed with:\n",
    "\n",
    "$$ error=actualâˆ’predicted $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting output for me is around ~405k.\n",
    "\n",
    "*Note*: This value depends on the train and the test set, which were splitted randomly. Therefore, it is normal that the value you obtained is different from mine or your neighbour's.\n",
    "\n",
    "This result can be read as follows:\n",
    "\n",
    "**The average error is 405 000 dollars by house.**\n",
    "\n",
    "Is it high? We can only know that if we compare to the average house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_data[\"Price\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_price = melbourne_data[\"Price\"].describe()['mean']\n",
    "\n",
    "print(f\"The error with test data is ~{round(mae / mean_price * 100, 2)}% \"\n",
    "       \"of average home value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model\n",
    "\n",
    "How to make the model *more accurate*?\n",
    "\n",
    "First, look into [sklearn DecisionTreeRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). There is a lot of customizable parameters. \n",
    "\n",
    "One of them is `max_depth`, which default is `None`.\n",
    "\n",
    "As you saw with the visualization, our tree can get deep really quickly. If the tree's depth is $n$, the number of leaves will be $2^n$.\n",
    "\n",
    "This is a phenomenon called **overfitting**, where a model matches the training data almost perfectly, but does poorly in validation and other new data. It usually means the model is learning on **noise**, data that are details that should have been discarded in the learning phase. \n",
    "\n",
    "On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups. It fails to capture the underlying trend of the data. This other extreme is called **underfitting**.\n",
    "\n",
    "Examples:\n",
    "\n",
    "![Appropriate fitting](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/overfitting_2.png)\n",
    "\n",
    "Ironically, an accuracy of 100% is bad because it is often a sign of *overfit*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid over- and under- fitting?\n",
    "\n",
    "Underfitting can be avoided by:\n",
    "* Using more data\n",
    "* Use a more complex, non-linear algorithm.\n",
    "\n",
    "Overfitting can be avoided by:\n",
    "* **Early stopping**. In our tree case, it means lower the tree's depth, or the tree's leaves. In other cases, it usually means reducing the number of learning iterations.\n",
    "* **Pruning**. It means let the tree grows in a certain depth, but then cutting some branches / trim down some leaves that do not add much predictive power.\n",
    "\n",
    "But how do you know if you have a **good fit** ?\n",
    "\n",
    "Usually, the goal is to find the spot between overfitting and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae_from_depth(max_depth):\n",
    "    melbourne_model = DecisionTreeRegressor(max_depth=max_depth, random_state=1)\n",
    "    trained_tree = melbourne_model.fit(X_train, y_train)\n",
    "    predicted_home_prices = melbourne_model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, predicted_home_prices)\n",
    "\n",
    "for max_depth in [5, 7, 10, 11, 12, 13, 14, 15, 20, 50, 100]:\n",
    "    mae = get_mae_from_depth(max_depth)\n",
    "    print(f\"MAE for max_depth={max_depth:3}: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the minimum MAE is around a max_depth of **12**.\n",
    "\n",
    "Now, let's do something similar with the number of leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae_from_leaves(max_leaf_nodes):\n",
    "    melbourne_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n",
    "    trained_tree = melbourne_model.fit(X_train, y_train)\n",
    "    predicted_home_prices = melbourne_model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, predicted_home_prices)\n",
    "\n",
    "for max_leaf_nodes in [5, 10, 25, 50, 100, 150, 200, 250, 300, 350, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "    mae = get_mae_from_leaves(max_leaf_nodes)\n",
    "    print(f\"MAE for max_leaf_nodes={max_leaf_nodes:3}: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the minimum MAE kinda ondulates. \n",
    "\n",
    "### Visualize the error\n",
    "\n",
    "Let's visualize it to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_scores = pd.DataFrame(range(20, 1000, 20), columns=[\"max_leaves\"])\n",
    "mae_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_scores[\"mae_leaves\"] = mae_scores[\"max_leaves\"].apply(get_mae_from_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=\"max_leaves\", y=\"mae_leaves\", data=mae_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, great, but this graph is too tiny. Let's also show the `xticks` so we can see at which `x` the minimum MAE is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.set_xticks(mae_scores[\"max_leaves\"])\n",
    "ax.scatter(x=\"max_leaves\", y=\"mae_leaves\", data=mae_scores)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's clear, the minimum MAE is clearer.\n",
    "\n",
    "For me it's ~324k MAE for a max_leaves of 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_scores[mae_scores['max_leaves'] == 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for max_depth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_score = pd.DataFrame(range(3, 50, 1), columns=[\"max_depth\"])\n",
    "depth_score[\"mae_depth\"] = depth_score[\"max_depth\"].apply(get_mae_from_depth)\n",
    "depth_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.set_xticks(depth_score[\"max_depth\"])\n",
    "ax.scatter(x=\"max_depth\", y=\"mae_depth\", data=depth_score, c='orange')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it's a ~317 MAE for a max_depth of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_score[depth_score['max_depth'] == 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for visual comparaison, we can put them on the same axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.set_xticks(range(0, 1000, 20))\n",
    "\n",
    "ax.scatter(x=\"max_leaves\", y=\"mae_leaves\", data=mae_scores)\n",
    "ax.scatter(x=\"max_depth\", y=\"mae_depth\", data=depth_score, c='orange')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change max_depth and max_leaf_nodes according to your result\n",
    "max_depth = 7\n",
    "max_leaf_nodes = 60\n",
    "\n",
    "# MAE with best max_depth\n",
    "melbourne_model = DecisionTreeRegressor(max_depth=max_depth, \n",
    "                                        random_state=1)\n",
    "trained_tree = melbourne_model.fit(X_train, y_train)\n",
    "predicted_home_prices = melbourne_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, predicted_home_prices)\n",
    "print(f\"MAE for max_depth={max_depth:3}: {mae}\")\n",
    "print(f\"The error with test data is ~{round(mae / mean_price * 100, 2)}% \"\n",
    "       \"of average home value.\")\n",
    "\n",
    "# MAE with best max_leaf_nodes\n",
    "mae = get_mae_from_leaves(max_leaf_nodes)\n",
    "print(f\"MAE for max_leaf_nodes={max_leaf_nodes:3}: {mae}\")\n",
    "\n",
    "print(f\"The error with test data is ~{round(mae / mean_price * 100, 2)}% \"\n",
    "       \"of average home value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already supported by our visualizations, we got the minimum error for a `max_depth` of **7**. \n",
    "\n",
    "It's still not perfect, but we reduced our original error of ~405k to ~316k, which represents a **22% error reduction**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
